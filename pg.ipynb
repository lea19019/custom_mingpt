{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "07936586",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import functional as F\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee18097",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PowBackward0 object at 0x7fe72c08aa70>\n"
     ]
    }
   ],
   "source": [
    "# Create tensors and tell PyTorch to track operations on them\n",
    "x = torch.tensor([2.0], requires_grad=True)\n",
    "w = torch.tensor([3.0], requires_grad=True)\n",
    "b = torch.tensor([1.0], requires_grad=True)\n",
    "\n",
    "# Do some operations\n",
    "y = w * x + b  # y = 3*2 + 1 = 7\n",
    "loss = y ** 2  # loss = 49\n",
    "\n",
    "print(loss.grad_fn)  # <PowBackward0 ...>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2986337c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([42.])\n",
      "tensor([28.])\n",
      "tensor([14.])\n",
      "tensor([49.], grad_fn=<PowBackward0>)\n"
     ]
    }
   ],
   "source": [
    "loss.backward()  # Compute gradients\n",
    "print(x.grad)  # dy/dx = 2*y*w = 2*7*3 = 42\n",
    "print(w.grad)  # dy/dw = 2*y*x = 2*7*2 = 28\n",
    "print(b.grad)  # dy/db = 2*y = 2*7 = 14 \n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cd7d8935",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "tensor([6., 4.])\n"
     ]
    }
   ],
   "source": [
    "w = torch.tensor([3.0, 2.0], requires_grad=True)\n",
    "loss = (w ** 2).sum() # Now loss = 9 + 4 = 13 (scalar!)\n",
    "\n",
    "# Before backward():\n",
    "print(w.grad)  # None (no gradient computed yet)\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "# After backward():\n",
    "print(w.grad) # tensor([6.0, 4.0]) (gradient computed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4280370f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 124.05M\n"
     ]
    }
   ],
   "source": [
    "from mingpt.model import GPT\n",
    "model_config = GPT.get_default_config()\n",
    "model_config.model_type = 'gpt2'\n",
    "model_config.vocab_size = 50257\n",
    "model_config.block_size = 512\n",
    "model = GPT(model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "126a528d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformer.wte.weight torch.Size([50257, 768]) True\n",
      "transformer.wpe.weight torch.Size([512, 768]) True\n",
      "transformer.h.0.ln_1.weight torch.Size([768]) True\n",
      "transformer.h.0.ln_1.bias torch.Size([768]) True\n",
      "transformer.h.0.attn.c_attn.weight torch.Size([2304, 768]) True\n",
      "transformer.h.0.attn.c_attn.bias torch.Size([2304]) True\n",
      "transformer.h.0.attn.c_proj.weight torch.Size([768, 768]) True\n",
      "transformer.h.0.attn.c_proj.bias torch.Size([768]) True\n",
      "transformer.h.0.ln_2.weight torch.Size([768]) True\n",
      "transformer.h.0.ln_2.bias torch.Size([768]) True\n",
      "transformer.h.0.mlp.c_fc.weight torch.Size([3072, 768]) True\n",
      "transformer.h.0.mlp.c_fc.bias torch.Size([3072]) True\n",
      "transformer.h.0.mlp.c_proj.weight torch.Size([768, 3072]) True\n",
      "transformer.h.0.mlp.c_proj.bias torch.Size([768]) True\n",
      "transformer.h.1.ln_1.weight torch.Size([768]) True\n",
      "transformer.h.1.ln_1.bias torch.Size([768]) True\n",
      "transformer.h.1.attn.c_attn.weight torch.Size([2304, 768]) True\n",
      "transformer.h.1.attn.c_attn.bias torch.Size([2304]) True\n",
      "transformer.h.1.attn.c_proj.weight torch.Size([768, 768]) True\n",
      "transformer.h.1.attn.c_proj.bias torch.Size([768]) True\n",
      "transformer.h.1.ln_2.weight torch.Size([768]) True\n",
      "transformer.h.1.ln_2.bias torch.Size([768]) True\n",
      "transformer.h.1.mlp.c_fc.weight torch.Size([3072, 768]) True\n",
      "transformer.h.1.mlp.c_fc.bias torch.Size([3072]) True\n",
      "transformer.h.1.mlp.c_proj.weight torch.Size([768, 3072]) True\n",
      "transformer.h.1.mlp.c_proj.bias torch.Size([768]) True\n",
      "transformer.h.2.ln_1.weight torch.Size([768]) True\n",
      "transformer.h.2.ln_1.bias torch.Size([768]) True\n",
      "transformer.h.2.attn.c_attn.weight torch.Size([2304, 768]) True\n",
      "transformer.h.2.attn.c_attn.bias torch.Size([2304]) True\n",
      "transformer.h.2.attn.c_proj.weight torch.Size([768, 768]) True\n",
      "transformer.h.2.attn.c_proj.bias torch.Size([768]) True\n",
      "transformer.h.2.ln_2.weight torch.Size([768]) True\n",
      "transformer.h.2.ln_2.bias torch.Size([768]) True\n",
      "transformer.h.2.mlp.c_fc.weight torch.Size([3072, 768]) True\n",
      "transformer.h.2.mlp.c_fc.bias torch.Size([3072]) True\n",
      "transformer.h.2.mlp.c_proj.weight torch.Size([768, 3072]) True\n",
      "transformer.h.2.mlp.c_proj.bias torch.Size([768]) True\n",
      "transformer.h.3.ln_1.weight torch.Size([768]) True\n",
      "transformer.h.3.ln_1.bias torch.Size([768]) True\n",
      "transformer.h.3.attn.c_attn.weight torch.Size([2304, 768]) True\n",
      "transformer.h.3.attn.c_attn.bias torch.Size([2304]) True\n",
      "transformer.h.3.attn.c_proj.weight torch.Size([768, 768]) True\n",
      "transformer.h.3.attn.c_proj.bias torch.Size([768]) True\n",
      "transformer.h.3.ln_2.weight torch.Size([768]) True\n",
      "transformer.h.3.ln_2.bias torch.Size([768]) True\n",
      "transformer.h.3.mlp.c_fc.weight torch.Size([3072, 768]) True\n",
      "transformer.h.3.mlp.c_fc.bias torch.Size([3072]) True\n",
      "transformer.h.3.mlp.c_proj.weight torch.Size([768, 3072]) True\n",
      "transformer.h.3.mlp.c_proj.bias torch.Size([768]) True\n",
      "transformer.h.4.ln_1.weight torch.Size([768]) True\n",
      "transformer.h.4.ln_1.bias torch.Size([768]) True\n",
      "transformer.h.4.attn.c_attn.weight torch.Size([2304, 768]) True\n",
      "transformer.h.4.attn.c_attn.bias torch.Size([2304]) True\n",
      "transformer.h.4.attn.c_proj.weight torch.Size([768, 768]) True\n",
      "transformer.h.4.attn.c_proj.bias torch.Size([768]) True\n",
      "transformer.h.4.ln_2.weight torch.Size([768]) True\n",
      "transformer.h.4.ln_2.bias torch.Size([768]) True\n",
      "transformer.h.4.mlp.c_fc.weight torch.Size([3072, 768]) True\n",
      "transformer.h.4.mlp.c_fc.bias torch.Size([3072]) True\n",
      "transformer.h.4.mlp.c_proj.weight torch.Size([768, 3072]) True\n",
      "transformer.h.4.mlp.c_proj.bias torch.Size([768]) True\n",
      "transformer.h.5.ln_1.weight torch.Size([768]) True\n",
      "transformer.h.5.ln_1.bias torch.Size([768]) True\n",
      "transformer.h.5.attn.c_attn.weight torch.Size([2304, 768]) True\n",
      "transformer.h.5.attn.c_attn.bias torch.Size([2304]) True\n",
      "transformer.h.5.attn.c_proj.weight torch.Size([768, 768]) True\n",
      "transformer.h.5.attn.c_proj.bias torch.Size([768]) True\n",
      "transformer.h.5.ln_2.weight torch.Size([768]) True\n",
      "transformer.h.5.ln_2.bias torch.Size([768]) True\n",
      "transformer.h.5.mlp.c_fc.weight torch.Size([3072, 768]) True\n",
      "transformer.h.5.mlp.c_fc.bias torch.Size([3072]) True\n",
      "transformer.h.5.mlp.c_proj.weight torch.Size([768, 3072]) True\n",
      "transformer.h.5.mlp.c_proj.bias torch.Size([768]) True\n",
      "transformer.h.6.ln_1.weight torch.Size([768]) True\n",
      "transformer.h.6.ln_1.bias torch.Size([768]) True\n",
      "transformer.h.6.attn.c_attn.weight torch.Size([2304, 768]) True\n",
      "transformer.h.6.attn.c_attn.bias torch.Size([2304]) True\n",
      "transformer.h.6.attn.c_proj.weight torch.Size([768, 768]) True\n",
      "transformer.h.6.attn.c_proj.bias torch.Size([768]) True\n",
      "transformer.h.6.ln_2.weight torch.Size([768]) True\n",
      "transformer.h.6.ln_2.bias torch.Size([768]) True\n",
      "transformer.h.6.mlp.c_fc.weight torch.Size([3072, 768]) True\n",
      "transformer.h.6.mlp.c_fc.bias torch.Size([3072]) True\n",
      "transformer.h.6.mlp.c_proj.weight torch.Size([768, 3072]) True\n",
      "transformer.h.6.mlp.c_proj.bias torch.Size([768]) True\n",
      "transformer.h.7.ln_1.weight torch.Size([768]) True\n",
      "transformer.h.7.ln_1.bias torch.Size([768]) True\n",
      "transformer.h.7.attn.c_attn.weight torch.Size([2304, 768]) True\n",
      "transformer.h.7.attn.c_attn.bias torch.Size([2304]) True\n",
      "transformer.h.7.attn.c_proj.weight torch.Size([768, 768]) True\n",
      "transformer.h.7.attn.c_proj.bias torch.Size([768]) True\n",
      "transformer.h.7.ln_2.weight torch.Size([768]) True\n",
      "transformer.h.7.ln_2.bias torch.Size([768]) True\n",
      "transformer.h.7.mlp.c_fc.weight torch.Size([3072, 768]) True\n",
      "transformer.h.7.mlp.c_fc.bias torch.Size([3072]) True\n",
      "transformer.h.7.mlp.c_proj.weight torch.Size([768, 3072]) True\n",
      "transformer.h.7.mlp.c_proj.bias torch.Size([768]) True\n",
      "transformer.h.8.ln_1.weight torch.Size([768]) True\n",
      "transformer.h.8.ln_1.bias torch.Size([768]) True\n",
      "transformer.h.8.attn.c_attn.weight torch.Size([2304, 768]) True\n",
      "transformer.h.8.attn.c_attn.bias torch.Size([2304]) True\n",
      "transformer.h.8.attn.c_proj.weight torch.Size([768, 768]) True\n",
      "transformer.h.8.attn.c_proj.bias torch.Size([768]) True\n",
      "transformer.h.8.ln_2.weight torch.Size([768]) True\n",
      "transformer.h.8.ln_2.bias torch.Size([768]) True\n",
      "transformer.h.8.mlp.c_fc.weight torch.Size([3072, 768]) True\n",
      "transformer.h.8.mlp.c_fc.bias torch.Size([3072]) True\n",
      "transformer.h.8.mlp.c_proj.weight torch.Size([768, 3072]) True\n",
      "transformer.h.8.mlp.c_proj.bias torch.Size([768]) True\n",
      "transformer.h.9.ln_1.weight torch.Size([768]) True\n",
      "transformer.h.9.ln_1.bias torch.Size([768]) True\n",
      "transformer.h.9.attn.c_attn.weight torch.Size([2304, 768]) True\n",
      "transformer.h.9.attn.c_attn.bias torch.Size([2304]) True\n",
      "transformer.h.9.attn.c_proj.weight torch.Size([768, 768]) True\n",
      "transformer.h.9.attn.c_proj.bias torch.Size([768]) True\n",
      "transformer.h.9.ln_2.weight torch.Size([768]) True\n",
      "transformer.h.9.ln_2.bias torch.Size([768]) True\n",
      "transformer.h.9.mlp.c_fc.weight torch.Size([3072, 768]) True\n",
      "transformer.h.9.mlp.c_fc.bias torch.Size([3072]) True\n",
      "transformer.h.9.mlp.c_proj.weight torch.Size([768, 3072]) True\n",
      "transformer.h.9.mlp.c_proj.bias torch.Size([768]) True\n",
      "transformer.h.10.ln_1.weight torch.Size([768]) True\n",
      "transformer.h.10.ln_1.bias torch.Size([768]) True\n",
      "transformer.h.10.attn.c_attn.weight torch.Size([2304, 768]) True\n",
      "transformer.h.10.attn.c_attn.bias torch.Size([2304]) True\n",
      "transformer.h.10.attn.c_proj.weight torch.Size([768, 768]) True\n",
      "transformer.h.10.attn.c_proj.bias torch.Size([768]) True\n",
      "transformer.h.10.ln_2.weight torch.Size([768]) True\n",
      "transformer.h.10.ln_2.bias torch.Size([768]) True\n",
      "transformer.h.10.mlp.c_fc.weight torch.Size([3072, 768]) True\n",
      "transformer.h.10.mlp.c_fc.bias torch.Size([3072]) True\n",
      "transformer.h.10.mlp.c_proj.weight torch.Size([768, 3072]) True\n",
      "transformer.h.10.mlp.c_proj.bias torch.Size([768]) True\n",
      "transformer.h.11.ln_1.weight torch.Size([768]) True\n",
      "transformer.h.11.ln_1.bias torch.Size([768]) True\n",
      "transformer.h.11.attn.c_attn.weight torch.Size([2304, 768]) True\n",
      "transformer.h.11.attn.c_attn.bias torch.Size([2304]) True\n",
      "transformer.h.11.attn.c_proj.weight torch.Size([768, 768]) True\n",
      "transformer.h.11.attn.c_proj.bias torch.Size([768]) True\n",
      "transformer.h.11.ln_2.weight torch.Size([768]) True\n",
      "transformer.h.11.ln_2.bias torch.Size([768]) True\n",
      "transformer.h.11.mlp.c_fc.weight torch.Size([3072, 768]) True\n",
      "transformer.h.11.mlp.c_fc.bias torch.Size([3072]) True\n",
      "transformer.h.11.mlp.c_proj.weight torch.Size([768, 3072]) True\n",
      "transformer.h.11.mlp.c_proj.bias torch.Size([768]) True\n",
      "transformer.ln_f.weight torch.Size([768]) True\n",
      "transformer.ln_f.bias torch.Size([768]) True\n",
      "lm_head.weight torch.Size([50257, 768]) True\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(name, param.shape, param.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd3aa5d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Example 1: Tensor shapes ===\n",
      "Original shape: torch.Size([2, 3])\n",
      "x.size(-1) = 3\n",
      "x.size(0) = 2\n",
      "\n",
      "x.view(-1) flattens to: tensor([1, 2, 3, 4, 5, 6])\n",
      "Shape: torch.Size([6])\n",
      "\n",
      "x.view(-1, 3) reshapes to:\n",
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n",
      "Shape: torch.Size([2, 3])\n",
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n",
      "\n",
      "=== Example 2: Cross Entropy Requirements ===\n",
      "logits shape: torch.Size([2, 3, 5])\n",
      "targets shape: torch.Size([2, 3])\n",
      "\n",
      "logits.view(-1, logits.size(-1)) shape: torch.Size([6, 5])\n",
      "targets.view(-1) shape: torch.Size([6])\n",
      "\n",
      "Loss: 1.7956656217575073\n",
      "tensor([[[-0.0573, -0.2409,  0.0426, -0.3903, -1.1125],\n",
      "         [ 1.2622, -0.1017, -0.2237,  0.1274,  1.8104],\n",
      "         [ 1.9876, -0.2748, -0.8699,  1.4689,  0.1783]],\n",
      "\n",
      "        [[-1.5124, -1.3039, -0.9486,  0.8336, -0.1846],\n",
      "         [ 0.0790,  0.5465,  0.7917,  0.4532,  0.3316],\n",
      "         [ 1.4380, -1.6008,  1.8799, -0.1534, -0.5787]]])\n",
      "tensor([[1, 2, 3],\n",
      "        [0, 4, 2]])\n",
      "tensor([[-0.0573, -0.2409,  0.0426, -0.3903, -1.1125],\n",
      "        [ 1.2622, -0.1017, -0.2237,  0.1274,  1.8104],\n",
      "        [ 1.9876, -0.2748, -0.8699,  1.4689,  0.1783],\n",
      "        [-1.5124, -1.3039, -0.9486,  0.8336, -0.1846],\n",
      "        [ 0.0790,  0.5465,  0.7917,  0.4532,  0.3316],\n",
      "        [ 1.4380, -1.6008,  1.8799, -0.1534, -0.5787]])\n",
      "tensor([1, 2, 3, 0, 4, 2])\n",
      "\n",
      "=== Example 3: ignore_index ===\n",
      "Loss (ignoring -1 tokens): 1.9642508029937744\n",
      "The -1 positions don't contribute to the loss!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Example 1: Understanding .view() and .size()\n",
    "print(\"=== Example 1: Tensor shapes ===\")\n",
    "x = torch.tensor([[1, 2, 3],\n",
    "                  [4, 5, 6]])\n",
    "print(f\"Original shape: {x.shape}\")  # [2, 3]\n",
    "print(f\"x.size(-1) = {x.size(-1)}\")  # 3 (last dimension)\n",
    "print(f\"x.size(0) = {x.size(0)}\")    # 2 (first dimension)\n",
    "\n",
    "# .view() reshapes the tensor\n",
    "print(f\"\\nx.view(-1) flattens to: {x.view(-1)}\")  # [1, 2, 3, 4, 5, 6]\n",
    "print(f\"Shape: {x.view(-1).shape}\")  # [6]\n",
    "\n",
    "print(f\"\\nx.view(-1, 3) reshapes to:\\n{x.view(-1, 3)}\")  # Same as original\n",
    "print(f\"Shape: {x.view(-1, 3).shape}\")  # [2, 3]\n",
    "\n",
    "print(x)\n",
    "\n",
    "# Example 2: What cross_entropy actually needs\n",
    "print(\"\\n=== Example 2: Cross Entropy Requirements ===\")\n",
    "\n",
    "# Simulating your model's output\n",
    "batch_size = 2\n",
    "seq_len = 3\n",
    "vocab_size = 5\n",
    "\n",
    "# logits shape: (batch, sequence, vocab) - predictions for each position\n",
    "logits = torch.randn(batch_size, seq_len, vocab_size)\n",
    "print(f\"logits shape: {logits.shape}\")  # [2, 3, 5]\n",
    "\n",
    "# targets shape: (batch, sequence) - correct token at each position\n",
    "targets = torch.tensor([[1, 2, 3],\n",
    "                        [0, 4, 2]])\n",
    "print(f\"targets shape: {targets.shape}\")  # [2, 3]\n",
    "\n",
    "# Cross entropy wants: (N, C) for predictions and (N,) for targets\n",
    "# where N = number of predictions, C = number of classes\n",
    "logits_flat = logits.view(-1, logits.size(-1))\n",
    "print(f\"\\nlogits.view(-1, logits.size(-1)) shape: {logits_flat.shape}\")  # [6, 5]\n",
    "\n",
    "targets_flat = targets.view(-1)\n",
    "print(f\"targets.view(-1) shape: {targets_flat.shape}\")  # [6]\n",
    "\n",
    "loss = F.cross_entropy(logits_flat, targets_flat)\n",
    "print(f\"\\nLoss: {loss.item()}\")\n",
    "\n",
    "print(logits)\n",
    "print(targets)\n",
    "print(logits_flat)\n",
    "print(targets_flat)\n",
    "\n",
    "# Example 3: What does ignore_index do?\n",
    "print(\"\\n=== Example 3: ignore_index ===\")\n",
    "\n",
    "targets_with_padding = torch.tensor([[1, 2, -1],  # -1 = padding token\n",
    "                                      [0, -1, 2]])  # -1 = padding token\n",
    "\n",
    "loss_with_ignore = F.cross_entropy(logits_flat, targets_with_padding.view(-1), \n",
    "                                     ignore_index=-1)\n",
    "print(f\"Loss (ignoring -1 tokens): {loss_with_ignore.item()}\")\n",
    "print(\"The -1 positions don't contribute to the loss!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e8ef78c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shape: torch.Size([2, 3, 5])\n",
      "logits.size(-1) = 5\n",
      "\n",
      "Flattened shape: torch.Size([6, 5])\n",
      "Flattened tensor:\n",
      "tensor([[ 1,  2,  3,  4,  5],\n",
      "        [ 6,  7,  8,  9, 10],\n",
      "        [11, 12, 13, 14, 15],\n",
      "        [16, 17, 18, 19, 20],\n",
      "        [21, 22, 23, 24, 25],\n",
      "        [26, 27, 28, 29, 30]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Start with a 3D tensor\n",
    "logits = torch.tensor([\n",
    "    [[1, 2, 3, 4, 5],    # batch 0, position 0, vocab scores\n",
    "     [6, 7, 8, 9, 10],   # batch 0, position 1, vocab scores\n",
    "     [11, 12, 13, 14, 15]], # batch 0, position 2, vocab scores\n",
    "    \n",
    "    [[16, 17, 18, 19, 20],  # batch 1, position 0, vocab scores\n",
    "     [21, 22, 23, 24, 25],  # batch 1, position 1, vocab scores\n",
    "     [26, 27, 28, 29, 30]]  # batch 1, position 2, vocab scores\n",
    "])\n",
    "\n",
    "print(f\"Original shape: {logits.shape}\")  # [2, 3, 5]\n",
    "print(f\"logits.size(-1) = {logits.size(-1)}\")  # 5\n",
    "\n",
    "# Now reshape\n",
    "flat = logits.view(-1, logits.size(-1))\n",
    "print(f\"\\nFlattened shape: {flat.shape}\")  # [6, 5]\n",
    "print(f\"Flattened tensor:\\n{flat}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef108a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(2, 3, 5)  # 30 elements total\n",
    "\n",
    "# All of these give the same result:\n",
    "print(x.view(-1, 5).shape)    # [6, 5]\n",
    "print(x.view(6, -1).shape)    # [6, 5]\n",
    "print(x.view(2, -1).shape)    # [2, 15]\n",
    "print(x.view(-1).shape)       # [30] - fully flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "abec9eba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([4, 5, 6])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([[1, 2, 3],\n",
    "                  [4, 5, 6]])\n",
    "print(x.shape)\n",
    "x[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "56c35d7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits shape: torch.Size([1, 5])\n",
      "target shape: torch.Size([1])\n",
      "Loss: 0.1983986645936966\n",
      "probs:  tensor([[0.1356, 0.0030, 0.0302, 0.8200, 0.0111]])\n",
      "probs shape:  torch.Size([1, 5])\n",
      "Probability of correct token (index 3): 0.8200\n",
      "-log of that probability: 0.1984\n",
      "\n",
      "exp(logits): tensor([[ 9.9742,  0.2231,  2.2255, 60.3403,  0.8187]])\n",
      "exp(2.3) = 9.974182454814718\n",
      "exp(4.1) = 60.34028759736195\n",
      "Sum of all exp: tensor(73.5819)\n",
      "Probabilities: tensor([[0.1356, 0.0030, 0.0302, 0.8200, 0.0111]])\n",
      "\n",
      "Loss when model is wrong: 1.9983986616134644\n"
     ]
    }
   ],
   "source": [
    "logits = torch.tensor([[2.3, -1.5, 0.8, 4.1, -0.2]])  # scores for 5 tokens\n",
    "target = torch.tensor([3])  # correct token is index 3\n",
    "print('logits shape:', logits.shape)\n",
    "print('target shape:', target.shape)\n",
    "loss = F.cross_entropy(logits, target)\n",
    "print(f\"Loss: {loss.item()}\")\n",
    "\n",
    "# What happened?\n",
    "probs = F.softmax(logits, dim=-1)\n",
    "print('probs: ',probs)\n",
    "print('probs shape: ', probs.shape)\n",
    "print(f\"Probability of correct token (index 3): {probs[0, 3].item():.4f}\")\n",
    "print(f\"-log of that probability: {-torch.log(probs[0, 3]).item():.4f}\")\n",
    "\n",
    "\n",
    "# Manual softmax calculation\n",
    "exp_logits = torch.exp(logits)\n",
    "print(\"\\nexp(logits):\", exp_logits)\n",
    "print(\"exp(2.3) =\", math.exp(2.3))\n",
    "print(\"exp(4.1) =\", math.exp(4.1))\n",
    "sum_exp = exp_logits.sum()\n",
    "print(\"Sum of all exp:\", sum_exp)\n",
    "probs = exp_logits / sum_exp\n",
    "print(\"Probabilities:\", probs)\n",
    "\n",
    "\n",
    "# What if model was wrong?\n",
    "bad_logits = torch.tensor([[4.1, -1.5, 0.8, 2.3, -0.2]])  # now best score at index 0\n",
    "target = torch.tensor([3])  # but correct is still index 3\n",
    "\n",
    "loss = F.cross_entropy(bad_logits, target)\n",
    "print(f\"\\nLoss when model is wrong: {loss.item()}\")  # Much higher!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf1dfad3",
   "metadata": {},
   "source": [
    "# Complete Summary: Understanding GPT and the Training Pipeline\n",
    "\n",
    "## Part 1: The Data Pipeline\n",
    "\n",
    "### From Text to Tokens\n",
    "\n",
    "**Tokenization** converts text into sequences of integers using Byte Pair Encoding (BPE):\n",
    "- Text: \"Hello world\" → Tokens: [15496, 995]\n",
    "- Each token ID represents a subword unit from a vocabulary of 50,257 possible tokens\n",
    "- Longer texts produce more tokens (variable length sequences)\n",
    "\n",
    "### Creating Training Pairs\n",
    "\n",
    "For next-token prediction, we create input/target pairs:\n",
    "```python\n",
    "tokens = [15496, 995, 13, 220, ...]  # Full tokenized sequence\n",
    "\n",
    "# Create shifted sequences for next-token prediction\n",
    "x = tokens[:-1]  # Input: all tokens except the last\n",
    "y = tokens[1:]   # Target: all tokens except the first\n",
    "\n",
    "# Example alignment:\n",
    "# x[0] = token_0, y[0] = token_1  (predict token_1 given token_0)\n",
    "# x[1] = token_1, y[1] = token_2  (predict token_2 given token_1)\n",
    "# ... and so on\n",
    "```\n",
    "\n",
    "**Key insight:** We predict the next token at every position in parallel.\n",
    "\n",
    "### Batching\n",
    "\n",
    "The DataLoader stacks multiple sequences together:\n",
    "- Individual sequence: `[seq_len]`\n",
    "- Batch of 8 sequences: `[8, seq_len]`\n",
    "- All sequences in a batch processed in parallel\n",
    "\n",
    "### Block Size Constraint\n",
    "```python\n",
    "block_size = 512  # Maximum sequence length\n",
    "\n",
    "if len(tokens) > block_size + 1:\n",
    "    tokens = tokens[:block_size + 1]  # Truncate if too long\n",
    "```\n",
    "\n",
    "This is a hard computational limit - the model can only process up to 512 tokens at once.\n",
    "\n",
    "---\n",
    "\n",
    "## Part 2: Embeddings - Converting Tokens to Vectors\n",
    "\n",
    "### Token Embeddings (wte - Word Token Embedding)\n",
    "\n",
    "**What:** A lookup table that converts token IDs to dense vectors\n",
    "```python\n",
    "wte = nn.Embedding(vocab_size=50257, embedding_dim=768)\n",
    "# Creates a matrix of shape [50257, 768]\n",
    "# Each row is a learned 768-dimensional vector for one token\n",
    "```\n",
    "\n",
    "**How it works:**\n",
    "```python\n",
    "idx = torch.tensor([[48, 25, 198]])  # Token IDs, shape [1, 3]\n",
    "tok_emb = wte(idx)                    # Look up embeddings, shape [1, 3, 768]\n",
    "\n",
    "# Internally:\n",
    "# tok_emb[0, 0, :] = wte.weight[48, :]   # Row 48 from the table\n",
    "# tok_emb[0, 1, :] = wte.weight[25, :]   # Row 25 from the table\n",
    "# tok_emb[0, 2, :] = wte.weight[198, :]  # Row 198 from the table\n",
    "```\n",
    "\n",
    "**Key property:** The same token always gets the same vector from this table, regardless of where it appears in the sequence.\n",
    "\n",
    "### Positional Embeddings (wpe - Word Position Embedding)\n",
    "\n",
    "**The problem:** Attention mechanisms are order-blind. Without position information:\n",
    "- \"The dog bit the cat\" and \"The cat bit the dog\" would look identical\n",
    "- The model needs to know WHERE each token appears\n",
    "\n",
    "**Solution:** Add position-specific information\n",
    "```python\n",
    "wpe = nn.Embedding(block_size=512, embedding_dim=768)\n",
    "# Creates a matrix of shape [512, 768]\n",
    "# Each row is a learned 768-dimensional vector for one position\n",
    "\n",
    "pos = torch.arange(0, seq_len)  # [0, 1, 2, ..., seq_len-1]\n",
    "pos_emb = wpe(pos)              # Look up position embeddings\n",
    "```\n",
    "\n",
    "**Key insight:** Position embeddings are shared across all items in a batch (position 0 is position 0 for everyone).\n",
    "\n",
    "### Combining Embeddings\n",
    "```python\n",
    "x = tok_emb + pos_emb  # Element-wise addition\n",
    "```\n",
    "\n",
    "Now each token's vector contains BOTH:\n",
    "1. **What the token is** (from tok_emb)\n",
    "2. **Where it appears** (from pos_emb)\n",
    "\n",
    "Example: \"dog\" at position 0 gets a different combined vector than \"dog\" at position 4, even though the token is the same!\n",
    "\n",
    "### Dropout for Regularization\n",
    "```python\n",
    "x = dropout(x)  # Randomly set some values to 0 during training\n",
    "```\n",
    "\n",
    "Prevents overfitting by forcing the model not to rely on any single feature.\n",
    "\n",
    "---\n",
    "\n",
    "## Part 3: The Forward Pass Through GPT\n",
    "\n",
    "### Complete Flow\n",
    "```python\n",
    "def forward(self, idx, targets=None):\n",
    "    # idx shape: [batch_size, seq_len] - contains token IDs (integers)\n",
    "    \n",
    "    # Step 1: Create embeddings\n",
    "    tok_emb = self.transformer.wte(idx)  # [batch, seq_len, 768]\n",
    "    pos_emb = self.transformer.wpe(pos)  # [1, seq_len, 768]\n",
    "    x = self.transformer.drop(tok_emb + pos_emb)  # [batch, seq_len, 768]\n",
    "    \n",
    "    # Step 2: Process through transformer blocks (THE LEARNING HAPPENS HERE)\n",
    "    for block in self.transformer.h:  # 12 blocks in GPT-2\n",
    "        x = block(x)  # Each block: attention + feed-forward\n",
    "    \n",
    "    # Step 3: Final processing\n",
    "    x = self.transformer.ln_f(x)  # Layer normalization for stability\n",
    "    logits = self.lm_head(x)      # Project to vocabulary size\n",
    "    # logits shape: [batch, seq_len, 50257]\n",
    "    \n",
    "    # Step 4: Compute loss if targets provided\n",
    "    if targets is not None:\n",
    "        loss = F.cross_entropy(\n",
    "            logits.view(-1, 50257),  # Flatten to [batch*seq_len, 50257]\n",
    "            targets.view(-1)          # Flatten to [batch*seq_len]\n",
    "        )\n",
    "    \n",
    "    return logits, loss\n",
    "```\n",
    "\n",
    "### Understanding Logits\n",
    "\n",
    "**Logits** are raw, unnormalized scores for each possible next token:\n",
    "- Shape: `[batch, seq_len, vocab_size]`\n",
    "- For each position, we have 50,257 scores (one per token in vocabulary)\n",
    "- Higher score = model thinks that token is more likely to come next\n",
    "\n",
    "**Not probabilities yet!** Cross entropy will apply softmax internally.\n",
    "\n",
    "---\n",
    "\n",
    "## Part 4: Loss Function - Cross Entropy\n",
    "\n",
    "### Why Cross Entropy?\n",
    "\n",
    "Language modeling is **multi-class classification** at each position:\n",
    "- Question: \"Which of 50,257 tokens comes next?\"\n",
    "- Answer: One specific token (discrete choice, not continuous)\n",
    "\n",
    "### How Cross Entropy Works\n",
    "```python\n",
    "# For one position:\n",
    "logits = [2.3, -1.5, 0.8, 4.1, -0.2]  # Raw scores\n",
    "target = 3  # Correct token is at index 3\n",
    "\n",
    "# Step 1: Convert logits to probabilities with softmax\n",
    "probs = softmax(logits)  # [0.14, 0.003, 0.03, 0.82, 0.01]\n",
    "# Higher logits → higher probabilities (exponentially)\n",
    "\n",
    "# Step 2: Compute loss\n",
    "loss = -log(probs[target])  # -log(0.82) = 0.20\n",
    "```\n",
    "\n",
    "**Interpretation:**\n",
    "- Model assigned 82% probability to correct token → low loss (good!)\n",
    "- If model assigned 10% → high loss (bad!)\n",
    "\n",
    "### For Multiple Positions\n",
    "```python\n",
    "logits.view(-1, 50257)   # [512, 50257] - 512 predictions\n",
    "targets.view(-1)          # [512] - 512 correct answers\n",
    "\n",
    "loss = F.cross_entropy(logits, targets)  # Average loss over all 512 positions\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Part 5: Training Loop Mechanics\n",
    "\n",
    "### PyTorch's Autograd - The Computational Graph\n",
    "\n",
    "When you perform operations on tensors with `requires_grad=True`, PyTorch builds a graph:\n",
    "```python\n",
    "w = torch.tensor([3.0], requires_grad=True)\n",
    "x = torch.tensor([2.0])\n",
    "y = w * x      # y = 6.0\n",
    "loss = y ** 2  # loss = 36.0\n",
    "\n",
    "# PyTorch remembers:\n",
    "# - loss came from y^2\n",
    "# - y came from w * x\n",
    "# - Therefore loss depends on w\n",
    "```\n",
    "\n",
    "Every tensor has a `.grad_fn` attribute that stores \"how was I created?\"\n",
    "\n",
    "### The Training Cycle\n",
    "```python\n",
    "# Forward pass\n",
    "logits, loss = model(x, y)  # Builds computational graph\n",
    "\n",
    "# Clear old gradients\n",
    "model.zero_grad()  # Sets all .grad attributes to None/zero\n",
    "\n",
    "# Backward pass\n",
    "loss.backward()    # Walks graph backwards, computes ∂loss/∂(every weight)\n",
    "                   # Stores gradients in weight.grad for each parameter\n",
    "\n",
    "# Update weights\n",
    "optimizer.step()   # For each weight: w = w - learning_rate * w.grad\n",
    "```\n",
    "\n",
    "**Key insight:** Only parameters that were actually used in computing the loss get gradient updates.\n",
    "\n",
    "### Gradient Clipping\n",
    "```python\n",
    "torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "```\n",
    "\n",
    "If gradients get too large (exploding gradients), scale them down for stability.\n",
    "\n",
    "### The Optimizer - AdamW\n",
    "```python\n",
    "optimizer = torch.optim.AdamW(parameters, lr=learning_rate)\n",
    "```\n",
    "\n",
    "AdamW is a sophisticated version of gradient descent that:\n",
    "- Adapts learning rate per parameter\n",
    "- Uses momentum (considers past gradients)\n",
    "- Applies weight decay for regularization\n",
    "\n",
    "---\n",
    "\n",
    "## Part 6: Key Concepts Recap\n",
    "\n",
    "### Iteration vs Epoch\n",
    "\n",
    "**One iteration:**\n",
    "- Process one batch\n",
    "- Compute loss\n",
    "- Backpropagate\n",
    "- Update weights once\n",
    "\n",
    "**One epoch:**\n",
    "- Process the entire dataset once\n",
    "- Multiple iterations per epoch (num_iterations = dataset_size / batch_size)\n",
    "\n",
    "### Shape Transformations\n",
    "\n",
    "Understanding tensor shapes is critical:\n",
    "```python\n",
    "# Start: [batch, seq_len] - token IDs (integers)\n",
    "idx = torch.Size([4, 512])\n",
    "\n",
    "# After token embedding: [batch, seq_len, n_embd]\n",
    "tok_emb = torch.Size([4, 512, 768])\n",
    "\n",
    "# After blocks: [batch, seq_len, n_embd] - same shape, refined representations\n",
    "x = torch.Size([4, 512, 768])\n",
    "\n",
    "# After lm_head: [batch, seq_len, vocab_size]\n",
    "logits = torch.Size([4, 512, 50257])\n",
    "\n",
    "# For loss computation: flatten batch and sequence dimensions\n",
    "logits_flat = torch.Size([2048, 50257])  # 4 * 512 = 2048 predictions\n",
    "targets_flat = torch.Size([2048])         # 2048 correct answers\n",
    "```\n",
    "\n",
    "### The `view` Operation\n",
    "\n",
    "Reshaping tensors without changing data:\n",
    "```python\n",
    "x = torch.randn(2, 3, 5)  # Shape [2, 3, 5], total 30 elements\n",
    "\n",
    "x.view(-1)         # [30] - fully flatten\n",
    "x.view(-1, 5)      # [6, 5] - PyTorch infers first dim: 30/5 = 6\n",
    "x.view(2, -1)      # [2, 15] - PyTorch infers second dim: 30/2 = 15\n",
    "x.view(6, 5)       # [6, 5] - explicit reshape\n",
    "```\n",
    "\n",
    "The `-1` means \"figure this dimension out automatically.\"\n",
    "\n",
    "---\n",
    "\n",
    "## Part 7: What Happens in the Blocks (High-Level)\n",
    "\n",
    "Before diving into code, understand the purpose:\n",
    "\n",
    "### Transformer Block Structure\n",
    "```python\n",
    "def forward(self, x):\n",
    "    # x shape: [batch, seq_len, 768]\n",
    "    \n",
    "    x = x + self.attn(self.ln_1(x))      # Attention: tokens communicate\n",
    "    x = x + self.mlpf(self.ln_2(x))      # Feed-forward: process information\n",
    "    \n",
    "    return x  # Same shape: [batch, seq_len, 768]\n",
    "```\n",
    "\n",
    "### 1. Attention Mechanism\n",
    "\n",
    "**Purpose:** Let tokens share information with each other\n",
    "\n",
    "- \"The cat sat on the mat\"\n",
    "- Token \"sat\" can attend to \"cat\" (who sat?) and \"mat\" (where?)\n",
    "- Each token gathers relevant context from other tokens\n",
    "- This is where GPT builds understanding of relationships\n",
    "\n",
    "### 2. Feed-Forward Network\n",
    "\n",
    "**Purpose:** Process the gathered information\n",
    "\n",
    "- After attention, each token has collected context\n",
    "- Feed-forward lets each token \"think\" about what it learned\n",
    "- Happens independently for each token (no communication here)\n",
    "\n",
    "### 3. Residual Connections (`x + ...`)\n",
    "\n",
    "**Purpose:** Help gradients flow during training\n",
    "\n",
    "- The `+` allows gradients to flow directly backward\n",
    "- Prevents vanishing gradient problem in deep networks\n",
    "- Also helps preserve information from earlier layers\n",
    "\n",
    "### 4. Layer Normalization\n",
    "\n",
    "**Purpose:** Stabilize training\n",
    "\n",
    "- Normalizes values to mean=0, std=1\n",
    "- Prevents values from exploding or vanishing\n",
    "- Applied before attention and feed-forward\n",
    "\n",
    "### Stack of 12 Blocks\n",
    "```python\n",
    "for block in self.transformer.h:  # Repeat 12 times\n",
    "    x = block(x)\n",
    "```\n",
    "\n",
    "Each pass:\n",
    "- Refines token representations\n",
    "- Builds deeper understanding\n",
    "- Incorporates more context\n",
    "\n",
    "By block 12, each token has a rich representation incorporating information from the entire sequence.\n",
    "\n",
    "---\n",
    "\n",
    "## Summary of the Complete Pipeline\n",
    "\n",
    "1. **Text → Tokens:** BPE tokenization converts text to integers\n",
    "2. **Tokens → Embeddings:** Look up learned vectors for tokens and positions\n",
    "3. **Preprocessing:** Combine embeddings, apply dropout\n",
    "4. **Transformer Blocks:** 12 layers of attention + feed-forward (THE MAGIC)\n",
    "5. **Output Projection:** Convert 768-dim vectors to 50,257-dim logit scores\n",
    "6. **Loss Computation:** Cross entropy measures prediction quality\n",
    "7. **Backpropagation:** Compute gradients for all parameters\n",
    "8. **Weight Update:** Optimizer adjusts parameters to reduce loss\n",
    "9. **Repeat:** Do this for thousands of iterations\n",
    "\n",
    "**You are here:** About to understand what happens inside those transformer blocks where the intelligence emerges!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6166aea0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "DEMONSTRATION 1: Embeddings as Lookup Tables\n",
      "============================================================\n",
      "\n",
      "Embedding table shape: torch.Size([10, 4])\n",
      "Embedding table (first 3 rows):\n",
      "tensor([[ 0.5441,  0.4206,  0.3946,  0.1132],\n",
      "        [ 0.2333,  0.4732,  0.2855,  1.1324],\n",
      "        [-0.6531,  0.2970, -0.0748, -0.8157]])\n",
      "\n",
      "Token IDs: tensor([2, 5, 2])\n",
      "Embedded shape: torch.Size([3, 4])\n",
      "\n",
      "Embedding for token 2 (first occurrence):\n",
      "tensor([-0.6531,  0.2970, -0.0748, -0.8157], grad_fn=<SelectBackward0>)\n",
      "Embedding for token 2 (second occurrence):\n",
      "tensor([-0.6531,  0.2970, -0.0748, -0.8157], grad_fn=<SelectBackward0>)\n",
      "They're identical! Same token = same embedding\n",
      "\n",
      "============================================================\n",
      "DEMONSTRATION 2: Softmax and Cross Entropy\n",
      "============================================================\n",
      "\n",
      "Logits: tensor([[ 2.3000, -1.5000,  0.8000,  4.1000, -0.2000]])\n",
      "Probabilities (after softmax): tensor([[0.1356, 0.0030, 0.0302, 0.8200, 0.0111]])\n",
      "Sum of probabilities: 1.0000\n",
      "\n",
      "Target token index: 3\n",
      "Probability assigned to correct token: 0.8200\n",
      "Cross entropy loss: 0.1984\n",
      "Manual calculation (-log(prob)): 0.1984\n",
      "\n",
      "============================================================\n",
      "DEMONSTRATION 3: Tensor Reshaping with view()\n",
      "============================================================\n",
      "Original shape: torch.Size([2, 3, 4])\n",
      "Total elements: 24\n",
      "\n",
      "x.view(-1) shape: torch.Size([24])\n",
      "x.view(-1, 4) shape: torch.Size([6, 4])\n",
      "x.view(2, -1) shape: torch.Size([2, 12])\n",
      "x.view(6, 4) shape: torch.Size([6, 4])\n",
      "\n",
      "============================================================\n",
      "DEMONSTRATION 4: Computational Graph and Gradients\n",
      "============================================================\n",
      "w = 3.0\n",
      "x = 2.0\n",
      "y = w * x = 6.0\n",
      "loss = y^2 = 36.0\n",
      "\n",
      "Before backward:\n",
      "w.grad = None\n",
      "\n",
      "After backward:\n",
      "w.grad = 24.0\n",
      "\n",
      "Math check: d(loss)/d(w) = d((w*x)^2)/d(w) = 2*(w*x)*x\n",
      "           = 2 * (3*2) * 2 = 24\n",
      "\n",
      "============================================================\n",
      "DEMONSTRATION 5: Batching and Broadcasting\n",
      "============================================================\n",
      "Token embeddings shape: torch.Size([2, 3, 4])\n",
      "Position embeddings shape: torch.Size([1, 3, 4])\n",
      "Combined shape: torch.Size([2, 3, 4])\n",
      "\n",
      "Position embedding is SHARED across batch items!\n",
      "\n",
      "============================================================\n",
      "All demonstrations complete!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Helper Code: Demonstrating Key Concepts\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"DEMONSTRATION 1: Embeddings as Lookup Tables\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create a tiny embedding table\n",
    "vocab_size = 10\n",
    "embedding_dim = 4\n",
    "embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "print(f\"\\nEmbedding table shape: {embedding.weight.shape}\")\n",
    "print(\"Embedding table (first 3 rows):\")\n",
    "print(embedding.weight[:3].detach())\n",
    "\n",
    "# Look up embeddings for tokens\n",
    "token_ids = torch.tensor([2, 5, 2])  # Note: token 2 appears twice\n",
    "embedded = embedding(token_ids)\n",
    "\n",
    "print(f\"\\nToken IDs: {token_ids}\")\n",
    "print(f\"Embedded shape: {embedded.shape}\")\n",
    "print(\"\\nEmbedding for token 2 (first occurrence):\")\n",
    "print(embedded[0])\n",
    "print(\"Embedding for token 2 (second occurrence):\")\n",
    "print(embedded[2])\n",
    "print(\"They're identical! Same token = same embedding\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"DEMONSTRATION 2: Softmax and Cross Entropy\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Simulated logits for 5 possible tokens\n",
    "logits = torch.tensor([[2.3, -1.5, 0.8, 4.1, -0.2]])\n",
    "target = torch.tensor([3])  # Correct token is at index 3\n",
    "\n",
    "# Manual softmax\n",
    "probs = F.softmax(logits, dim=-1)\n",
    "print(f\"\\nLogits: {logits}\")\n",
    "print(f\"Probabilities (after softmax): {probs}\")\n",
    "print(f\"Sum of probabilities: {probs.sum().item():.4f}\")\n",
    "\n",
    "# Cross entropy loss\n",
    "loss = F.cross_entropy(logits, target)\n",
    "manual_loss = -torch.log(probs[0, target])\n",
    "\n",
    "print(f\"\\nTarget token index: {target.item()}\")\n",
    "print(f\"Probability assigned to correct token: {probs[0, target].item():.4f}\")\n",
    "print(f\"Cross entropy loss: {loss.item():.4f}\")\n",
    "print(f\"Manual calculation (-log(prob)): {manual_loss.item():.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"DEMONSTRATION 3: Tensor Reshaping with view()\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create a sample tensor\n",
    "x = torch.arange(24).reshape(2, 3, 4)\n",
    "print(f\"Original shape: {x.shape}\")\n",
    "print(f\"Total elements: {x.numel()}\")\n",
    "\n",
    "print(f\"\\nx.view(-1) shape: {x.view(-1).shape}\")\n",
    "print(f\"x.view(-1, 4) shape: {x.view(-1, 4).shape}\")\n",
    "print(f\"x.view(2, -1) shape: {x.view(2, -1).shape}\")\n",
    "print(f\"x.view(6, 4) shape: {x.view(6, 4).shape}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"DEMONSTRATION 4: Computational Graph and Gradients\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Simple computation graph\n",
    "w = torch.tensor([3.0], requires_grad=True)\n",
    "x = torch.tensor([2.0])\n",
    "y = w * x\n",
    "loss = y ** 2\n",
    "\n",
    "print(f\"w = {w.item()}\")\n",
    "print(f\"x = {x.item()}\")\n",
    "print(f\"y = w * x = {y.item()}\")\n",
    "print(f\"loss = y^2 = {loss.item()}\")\n",
    "\n",
    "print(f\"\\nBefore backward:\")\n",
    "print(f\"w.grad = {w.grad}\")\n",
    "\n",
    "# Compute gradients\n",
    "loss.backward()\n",
    "\n",
    "print(f\"\\nAfter backward:\")\n",
    "print(f\"w.grad = {w.grad.item()}\")\n",
    "print(f\"\\nMath check: d(loss)/d(w) = d((w*x)^2)/d(w) = 2*(w*x)*x\")\n",
    "print(f\"           = 2 * (3*2) * 2 = {2 * (3*2) * 2}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"DEMONSTRATION 5: Batching and Broadcasting\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Token embeddings for a batch\n",
    "batch_size = 2\n",
    "seq_len = 3\n",
    "n_embd = 4\n",
    "\n",
    "tok_emb = torch.randn(batch_size, seq_len, n_embd)\n",
    "pos_emb = torch.randn(1, seq_len, n_embd)  # Note: batch dim is 1\n",
    "\n",
    "print(f\"Token embeddings shape: {tok_emb.shape}\")\n",
    "print(f\"Position embeddings shape: {pos_emb.shape}\")\n",
    "\n",
    "# Broadcasting: pos_emb is added to both items in batch\n",
    "combined = tok_emb + pos_emb\n",
    "\n",
    "print(f\"Combined shape: {combined.shape}\")\n",
    "print(\"\\nPosition embedding is SHARED across batch items!\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"All demonstrations complete!\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2334782",
   "metadata": {},
   "source": [
    "# Complete Summary: Deep Dive into Transformer Architecture and Modern Improvements\n",
    "\n",
    "## Part 8: Inside the Transformer Block - Where Intelligence Emerges\n",
    "\n",
    "### The Block Structure\n",
    "\n",
    "Each Transformer block performs two main operations with residual connections:\n",
    "\n",
    "**Residual Connections:**\n",
    "- Pattern: `output = input + transformation(input)`\n",
    "- Purpose: Allows gradients to flow directly backward through the `+` operation\n",
    "- Effect: Prevents vanishing gradients in deep networks (12 blocks)\n",
    "- Intuition: We don't replace the representation, we **refine** it incrementally\n",
    "\n",
    "**Layer Normalization:**\n",
    "- Applied before each sub-layer (pre-norm architecture)\n",
    "- Normalizes activations to have mean=0, std=1\n",
    "- Stabilizes training by preventing activations from exploding or vanishing\n",
    "- Includes learnable scale (gamma) and shift (beta) parameters\n",
    "\n",
    "---\n",
    "\n",
    "## Part 9: The Attention Mechanism - How Tokens Communicate\n",
    "\n",
    "### Core Purpose\n",
    "\n",
    "Attention allows each token to:\n",
    "1. Determine which other tokens are relevant to it\n",
    "2. Gather information from those relevant tokens\n",
    "3. Update its representation based on the gathered context\n",
    "\n",
    "### The Three Projections: Query, Key, Value\n",
    "\n",
    "From input embeddings `x` of shape `[batch, seq_len, n_embd]`, we create:\n",
    "\n",
    "**Query (Q):** Represents \"what information is this token seeking?\"\n",
    "**Key (K):** Represents \"what information does this token advertise?\"\n",
    "**Value (V):** Represents \"the actual information to transfer\"\n",
    "\n",
    "All three come from the same input, transformed by different learned weight matrices.\n",
    "\n",
    "### Mathematical Formulation\n",
    "\n",
    "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n",
    "\n",
    "**Breaking down the computation:**\n",
    "\n",
    "1. **Similarity Scores:** $QK^T$ computes dot products between all query-key pairs\n",
    "   - Shape: `[batch, seq_len, seq_len]`\n",
    "   - Entry `[i, j]` = how much token `i` should attend to token `j`\n",
    "\n",
    "2. **Scaling:** Divide by $\\sqrt{d_k}$ (square root of key dimension)\n",
    "   - Prevents dot products from getting too large\n",
    "   - Keeps softmax gradients stable\n",
    "\n",
    "3. **Attention Weights:** Apply softmax to convert scores to probabilities\n",
    "   - Each row sums to 1\n",
    "   - Represents distribution of attention across all tokens\n",
    "\n",
    "4. **Weighted Aggregation:** Multiply attention weights by values\n",
    "   - Each token's output = weighted average of all value vectors\n",
    "   - Weights determined by attention scores\n",
    "\n",
    "### Causal Masking\n",
    "\n",
    "For autoregressive language modeling, we must prevent tokens from attending to future positions:\n",
    "\n",
    "**Mechanism:** Set attention scores to $-\\infty$ for all positions $j > i$\n",
    "\n",
    "After softmax, $e^{-\\infty} = 0$, effectively blocking future information.\n",
    "\n",
    "**Implementation:** Lower triangular mask matrix\n",
    "\n",
    "$$\\text{mask} = \\begin{bmatrix}\n",
    "1 & 0 & 0 & 0 \\\\\n",
    "1 & 1 & 0 & 0 \\\\\n",
    "1 & 1 & 1 & 0 \\\\\n",
    "1 & 1 & 1 & 1\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "Where 0 positions become $-\\infty$ in attention scores.\n",
    "\n",
    "---\n",
    "\n",
    "## Part 10: Multi-Head Attention - Parallel Attention Patterns\n",
    "\n",
    "### Why Multiple Heads?\n",
    "\n",
    "Single attention lets the model learn one type of relationship. Multiple heads allow the model to simultaneously learn different types of patterns:\n",
    "- Head 1 might learn subject-verb relationships\n",
    "- Head 2 might learn adjective-noun relationships\n",
    "- Head 3 might learn long-range dependencies\n",
    "- etc.\n",
    "\n",
    "### Implementation via Tensor Reshaping\n",
    "\n",
    "Instead of running separate attention mechanisms, we split dimensions:\n",
    "\n",
    "**Process:**\n",
    "1. Split embedding dimension into `n_head` groups\n",
    "   - Original: `[batch, seq_len, 768]`\n",
    "   - Reshaped: `[batch, seq_len, n_head, head_dim]` where `head_dim = 768/12 = 64`\n",
    "\n",
    "2. Transpose to make heads the \"batch-like\" dimension\n",
    "   - Result: `[batch, n_head, seq_len, head_dim]`\n",
    "\n",
    "3. Compute attention independently for each head\n",
    "   - All heads processed in parallel via batched matrix operations\n",
    "\n",
    "4. Concatenate head outputs back together\n",
    "   - Transpose: `[batch, seq_len, n_head, head_dim]`\n",
    "   - Flatten: `[batch, seq_len, 768]`\n",
    "\n",
    "**Key insight:** This is data parallelism through tensor dimensions, not separate sequential operations. One matrix multiplication computes attention for all heads simultaneously.\n",
    "\n",
    "---\n",
    "\n",
    "## Part 11: The Feed-Forward Network (MLP)\n",
    "\n",
    "### Structure\n",
    "\n",
    "After attention, each token independently processes its updated representation:\n",
    "\n",
    "**Standard architecture:**\n",
    "```\n",
    "x → Linear(768 → 3072) → GELU → Linear(3072 → 768) → Dropout\n",
    "```\n",
    "\n",
    "**Expansion ratio:** 4× the embedding dimension (768 → 3072)\n",
    "\n",
    "### Purpose\n",
    "\n",
    "While attention mixes information **between** tokens, the feed-forward network processes information **within** each token:\n",
    "- Applies non-linear transformations\n",
    "- Provides computational capacity for complex pattern recognition\n",
    "- Operates independently on each position (no cross-token communication)\n",
    "\n",
    "### GELU Activation Function\n",
    "\n",
    "**Formula:**\n",
    "$$\\text{GELU}(x) = 0.5 \\cdot x \\cdot \\left(1 + \\tanh\\left(\\sqrt{\\frac{2}{\\pi}} \\left(x + 0.044715 x^3\\right)\\right)\\right)$$\n",
    "\n",
    "**Properties:**\n",
    "- Smooth approximation of ReLU\n",
    "- Allows small negative values through (unlike ReLU's hard cutoff at 0)\n",
    "- Better gradient flow during backpropagation\n",
    "- Prevents \"dying neuron\" problem\n",
    "\n",
    "**Why better than ReLU?**\n",
    "- ReLU: $f(x < 0) = 0$ → gradient is 0 → neuron can't recover\n",
    "- GELU: $f(x < 0) \\approx \\text{small negative}$ → gradient exists → neuron can learn\n",
    "\n",
    "---\n",
    "\n",
    "## Part 12: Modern Architectural Improvements\n",
    "\n",
    "### 1. RMSNorm - Efficient Normalization\n",
    "\n",
    "**LayerNorm performs two operations:**\n",
    "1. **Centering:** Subtract mean\n",
    "2. **Scaling:** Divide by standard deviation\n",
    "\n",
    "**RMSNorm simplifies to one operation:**\n",
    "- Skip mean subtraction (centering)\n",
    "- Only normalize by Root Mean Square (RMS)\n",
    "\n",
    "**Formula:**\n",
    "\n",
    "$$\\text{RMSNorm}(x) = \\frac{x}{\\sqrt{\\frac{1}{n}\\sum_{i=1}^n x_i^2 + \\epsilon}} \\cdot \\gamma$$\n",
    "\n",
    "Where:\n",
    "- $\\gamma$ is a learnable scaling parameter\n",
    "- $\\epsilon$ is a small constant for numerical stability (typically $10^{-8}$)\n",
    "\n",
    "**Benefits:**\n",
    "- 7-64% faster than LayerNorm (fewer operations)\n",
    "- Lower memory usage\n",
    "- Empirically performs as well or better\n",
    "- Used in modern LLMs (LLaMA, GPT-3)\n",
    "\n",
    "**Trade-off:** Slightly less theoretical grounding, but strong empirical results\n",
    "\n",
    "---\n",
    "\n",
    "### 2. SwiGLU - Gated Feed-Forward Networks\n",
    "\n",
    "**Standard MLP:**\n",
    "```\n",
    "x → Linear → Activation → Linear\n",
    "```\n",
    "\n",
    "**SwiGLU introduces gating:**\n",
    "```\n",
    "         ┌→ Linear1 → SiLU ──┐\n",
    "x → split                     × → Linear3 → output\n",
    "         └→ Linear2 ─────────┘\n",
    "```\n",
    "\n",
    "**Mathematical formulation:**\n",
    "\n",
    "$$\\text{SwiGLU}(x) = (xW_1 \\odot \\text{SiLU}(xW_2))W_3$$\n",
    "\n",
    "Where:\n",
    "- $W_1, W_2$: Parallel linear transformations (768 → 3072)\n",
    "- $\\text{SiLU}(x) = x \\cdot \\sigma(x)$: Sigmoid-weighted Linear Unit\n",
    "- $\\odot$: Element-wise multiplication (gating)\n",
    "- $W_3$: Output projection (3072 → 768)\n",
    "\n",
    "**Key mechanism:**\n",
    "- One path produces gate values (via SiLU activation)\n",
    "- Other path produces signal values\n",
    "- Element-wise multiplication allows adaptive feature selection\n",
    "\n",
    "**Benefits:**\n",
    "- Network learns which features to pass through based on context\n",
    "- Better gradient flow than standard activations\n",
    "- Improved performance in models like PaLM and LLaMA\n",
    "\n",
    "**Parameter increase:** Uses 3 linear layers instead of 2, increasing parameters by ~50% for the MLP\n",
    "\n",
    "---\n",
    "\n",
    "### 3. RoPE - Rotary Position Embeddings\n",
    "\n",
    "**Traditional approach:**\n",
    "- Learn absolute position embeddings\n",
    "- Add them to token embeddings at the start\n",
    "- Position info \"baked in\" to all subsequent operations\n",
    "\n",
    "**RoPE's innovation:**\n",
    "- No learned position embeddings\n",
    "- Apply position information as **rotation** to Q and K vectors\n",
    "- Rotation happens inside attention mechanism\n",
    "\n",
    "### The Rotation Concept\n",
    "\n",
    "**2D Rotation Matrix:**\n",
    "\n",
    "$$R(\\theta) = \\begin{bmatrix}\n",
    "\\cos(\\theta) & -\\sin(\\theta) \\\\\n",
    "\\sin(\\theta) & \\cos(\\theta)\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "**Properties:**\n",
    "- Preserves vector magnitude: $||R(\\theta)v|| = ||v||$\n",
    "- Changes direction by angle $\\theta$\n",
    "- Rotation is a multiplicative operation (matrix multiplication)\n",
    "\n",
    "**Key insight for attention:**\n",
    "\n",
    "If we rotate query at position $m$ by angle $m\\theta$ and key at position $n$ by angle $n\\theta$:\n",
    "\n",
    "$$Q_m^T K_n = (R_{m\\theta} q)^T (R_{n\\theta} k) = q^T R_{(n-m)\\theta} k$$\n",
    "\n",
    "The dot product naturally depends on $(n-m)$ - the **relative position difference**!\n",
    "\n",
    "### RoPE Implementation\n",
    "\n",
    "**Frequency bands:** Different dimension pairs rotate at different speeds\n",
    "\n",
    "$$\\theta_i = \\frac{m}{10000^{2i/d}}$$\n",
    "\n",
    "Where:\n",
    "- $m$ is the position index\n",
    "- $i$ is the dimension pair index\n",
    "- $d$ is the head dimension\n",
    "- Base of 10000 creates frequency spectrum\n",
    "\n",
    "**Rotation applied to dimension pairs:**\n",
    "\n",
    "For each pair of dimensions $(x_1, x_2)$ at position $m$:\n",
    "\n",
    "$$\\begin{bmatrix}\n",
    "x_1' \\\\\n",
    "x_2'\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "\\cos(m\\theta) & -\\sin(m\\theta) \\\\\n",
    "\\sin(m\\theta) & \\cos(m\\theta)\n",
    "\\end{bmatrix} \\begin{bmatrix}\n",
    "x_1 \\\\\n",
    "x_2\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "Simplifies to:\n",
    "- $x_1' = x_1 \\cos(m\\theta) - x_2 \\sin(m\\theta)$\n",
    "- $x_2' = x_1 \\sin(m\\theta) + x_2 \\cos(m\\theta)$\n",
    "\n",
    "**Benefits:**\n",
    "- Naturally encodes relative positions\n",
    "- No learned parameters for position\n",
    "- Better extrapolation to longer sequences than seen in training\n",
    "- Used in modern LLMs (LLaMA, GPT-NeoX)\n",
    "\n",
    "**Parameter reduction:** Removes learned positional embedding table (~0.4M parameters saved)\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Learning Rate Warmup\n",
    "\n",
    "**The problem:** At initialization, parameters are random and far from optimal. Large learning rates can cause:\n",
    "- Exploding gradients\n",
    "- Numerical instability\n",
    "- Divergent training\n",
    "\n",
    "**Solution:** Start with small learning rate, gradually increase to target value\n",
    "\n",
    "**Linear warmup formula:**\n",
    "\n",
    "$$\\text{lr}_t = \\text{lr}_{\\text{base}} \\cdot \\min\\left(1, \\frac{t}{T_{\\text{warmup}}}\\right)$$\n",
    "\n",
    "Where:\n",
    "- $t$ is the current step\n",
    "- $T_{\\text{warmup}}$ is the warmup period (e.g., 1000 steps)\n",
    "- Learning rate scales linearly from 0 to $\\text{lr}_{\\text{base}}$\n",
    "\n",
    "**Benefits:**\n",
    "- Prevents early training instability\n",
    "- Allows use of higher base learning rates\n",
    "- Smoother convergence\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Cosine Learning Rate Decay\n",
    "\n",
    "**The problem:** After warmup, maintaining constant learning rate can cause:\n",
    "- Oscillation around optimal values\n",
    "- Slower fine-grained convergence\n",
    "\n",
    "**Solution:** Gradually decrease learning rate following a cosine curve\n",
    "\n",
    "**Cosine annealing formula:**\n",
    "\n",
    "$$\\text{lr}_t = \\text{lr}_{\\text{min}} + \\frac{1}{2}(\\text{lr}_{\\text{max}} - \\text{lr}_{\\text{min}})\\left(1 + \\cos\\left(\\frac{\\pi t}{T_{\\text{max}}}\\right)\\right)$$\n",
    "\n",
    "Where:\n",
    "- $t$ is steps since warmup ended\n",
    "- $T_{\\text{max}}$ is total training steps\n",
    "- Learning rate smoothly decreases from $\\text{lr}_{\\text{max}}$ to $\\text{lr}_{\\text{min}}$\n",
    "\n",
    "**Combined warmup + cosine schedule:**\n",
    "1. Steps 0 to $T_{\\text{warmup}}$: Linear increase\n",
    "2. Steps $T_{\\text{warmup}}$ to $T_{\\text{max}}$: Cosine decay\n",
    "\n",
    "**Benefits:**\n",
    "- Smooth transitions (no sudden drops)\n",
    "- Model takes smaller steps near convergence\n",
    "- Better final performance\n",
    "- Standard in modern transformer training\n",
    "\n",
    "---\n",
    "\n",
    "## Part 13: Complete Forward Pass with All Improvements\n",
    "\n",
    "### Data Flow Through Modern Transformer\n",
    "\n",
    "**Input Processing:**\n",
    "1. Token IDs → Token embeddings (lookup table)\n",
    "2. If using RoPE: Skip positional embeddings\n",
    "3. If not using RoPE: Add learned positional embeddings\n",
    "4. Apply dropout for regularization\n",
    "\n",
    "**Through Each Block (×12):**\n",
    "1. **Layer Normalization** (RMSNorm or LayerNorm)\n",
    "   - Normalize before attention\n",
    "\n",
    "2. **Multi-Head Attention**\n",
    "   - Compute Q, K, V projections\n",
    "   - If using RoPE: Apply rotary embeddings to Q and K\n",
    "   - Compute attention scores with causal masking\n",
    "   - Weighted aggregation of values\n",
    "   - Output projection\n",
    "\n",
    "3. **Residual Connection**\n",
    "   - Add attention output to input\n",
    "\n",
    "4. **Layer Normalization** (RMSNorm or LayerNorm)\n",
    "   - Normalize before feed-forward\n",
    "\n",
    "5. **Feed-Forward Network**\n",
    "   - If using SwiGLU: Gated feed-forward with 3 linear layers\n",
    "   - If not: Standard MLP with 2 linear layers + GELU\n",
    "   - Apply dropout\n",
    "\n",
    "6. **Residual Connection**\n",
    "   - Add MLP output to input\n",
    "\n",
    "**Output Processing:**\n",
    "1. Final layer normalization\n",
    "2. Linear projection to vocabulary size (768 → 50,257)\n",
    "3. Output logits for each token position\n",
    "\n",
    "**Training:**\n",
    "1. Compute cross-entropy loss between logits and targets\n",
    "2. Backpropagate gradients through entire network\n",
    "3. Update weights using AdamW optimizer\n",
    "4. Step learning rate scheduler (warmup + cosine decay)\n",
    "\n",
    "---\n",
    "\n",
    "## Part 14: Tensor Shape Tracking Example\n",
    "\n",
    "### Concrete Example: Batch=2, SeqLen=512, Embedding=768, Heads=12\n",
    "\n",
    "**Input:**\n",
    "```\n",
    "Token IDs: [2, 512] (integers)\n",
    "```\n",
    "\n",
    "**After Embeddings:**\n",
    "```\n",
    "Token embeddings: [2, 512, 768]\n",
    "Position embeddings (if not RoPE): [1, 512, 768]\n",
    "Combined: [2, 512, 768]\n",
    "```\n",
    "\n",
    "**Inside Attention:**\n",
    "```\n",
    "Q, K, V after projection: [2, 512, 768] each\n",
    "\n",
    "After reshaping for multi-head:\n",
    "Q: [2, 512, 12, 64] → transpose → [2, 12, 512, 64]\n",
    "K: [2, 512, 12, 64] → transpose → [2, 12, 512, 64]\n",
    "V: [2, 512, 12, 64] → transpose → [2, 12, 512, 64]\n",
    "\n",
    "If using RoPE:\n",
    "  cos, sin: [512, 32] (half of head dimension)\n",
    "  Applied element-wise to rotated Q and K pairs\n",
    "\n",
    "Attention scores: Q @ K^T\n",
    "  [2, 12, 512, 64] @ [2, 12, 64, 512] = [2, 12, 512, 512]\n",
    "\n",
    "After causal mask and softmax: [2, 12, 512, 512]\n",
    "\n",
    "Attention output: scores @ V\n",
    "  [2, 12, 512, 512] @ [2, 12, 512, 64] = [2, 12, 512, 64]\n",
    "\n",
    "Concatenate heads:\n",
    "  transpose → [2, 512, 12, 64]\n",
    "  reshape → [2, 512, 768]\n",
    "```\n",
    "\n",
    "**Through Feed-Forward:**\n",
    "```\n",
    "If using SwiGLU:\n",
    "  Branch 1: [2, 512, 768] → Linear → [2, 512, 3072] → SiLU\n",
    "  Branch 2: [2, 512, 768] → Linear → [2, 512, 3072]\n",
    "  Gate: element-wise multiply → [2, 512, 3072]\n",
    "  Output projection: [2, 512, 3072] → Linear → [2, 512, 768]\n",
    "\n",
    "If using standard MLP:\n",
    "  [2, 512, 768] → Linear → [2, 512, 3072] → GELU → Linear → [2, 512, 768]\n",
    "```\n",
    "\n",
    "**Final Output:**\n",
    "```\n",
    "After 12 blocks: [2, 512, 768]\n",
    "After final norm: [2, 512, 768]\n",
    "After lm_head projection: [2, 512, 50257]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Part 15: Parameter Count Analysis\n",
    "\n",
    "### Baseline GPT-2 (124M parameters):\n",
    "\n",
    "**Embeddings:**\n",
    "- Token embeddings: 50,257 × 768 = 38.6M\n",
    "- Position embeddings: 512 × 768 = 0.4M\n",
    "\n",
    "**Per Transformer Block (×12):**\n",
    "- Attention Q,K,V projection: 768 × (3 × 768) = 1.8M\n",
    "- Attention output projection: 768 × 768 = 0.6M\n",
    "- MLP expand: 768 × 3072 = 2.4M\n",
    "- MLP contract: 3072 × 768 = 2.4M\n",
    "- LayerNorm (×2): ~3K parameters (negligible)\n",
    "- **Total per block: ~7.2M**\n",
    "- **12 blocks: ~86M**\n",
    "\n",
    "**Output:**\n",
    "- Final LayerNorm: ~1.5K\n",
    "- Language model head: shares weights with token embeddings\n",
    "\n",
    "**Total: ~124M parameters**\n",
    "\n",
    "### With All Modifications (152M parameters):\n",
    "\n",
    "**Changes:**\n",
    "- Remove position embeddings: -0.4M\n",
    "- Replace LayerNorm with RMSNorm: No parameter change\n",
    "- Replace MLP with SwiGLU: +~0.8M per block (third linear layer)\n",
    "- **Net increase: ~28M parameters (mostly from SwiGLU)**\n",
    "\n",
    "---\n",
    "\n",
    "## Part 16: Training Dynamics Summary\n",
    "\n",
    "### One Training Iteration:\n",
    "\n",
    "1. **Sample batch** from dataset (e.g., batch_size=8, seq_len=512)\n",
    "\n",
    "2. **Forward pass:**\n",
    "   - Compute token + position embeddings\n",
    "   - Pass through 12 transformer blocks\n",
    "   - Project to vocabulary logits\n",
    "   - Time complexity: $O(n \\cdot d^2 + n^2 \\cdot d)$ per block\n",
    "     - $n$ = sequence length\n",
    "     - $d$ = model dimension\n",
    "     - Attention: $O(n^2 \\cdot d)$ (quadratic in sequence length!)\n",
    "     - Feed-forward: $O(n \\cdot d^2)$ (linear in sequence length)\n",
    "\n",
    "3. **Compute loss:**\n",
    "   - Cross-entropy between logits and targets\n",
    "   - Average over batch and sequence positions\n",
    "\n",
    "4. **Backward pass:**\n",
    "   - Compute gradients via automatic differentiation\n",
    "   - Gradients flow through residual connections\n",
    "   - Clip gradient norms to prevent explosions\n",
    "\n",
    "5. **Update parameters:**\n",
    "   - AdamW optimizer adjusts all 124-152M parameters\n",
    "   - Learning rate determined by scheduler\n",
    "   - Weight decay applied to specific parameter groups\n",
    "\n",
    "6. **Step scheduler:**\n",
    "   - During warmup: increase learning rate\n",
    "   - After warmup: decrease via cosine schedule\n",
    "\n",
    "**Typical training:** Repeat for millions of iterations over billions of tokens\n",
    "\n",
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "### What Makes Transformers Work:\n",
    "\n",
    "1. **Attention mechanism:** Allows flexible, context-dependent information routing\n",
    "2. **Residual connections:** Enable training of very deep networks (12+ layers)\n",
    "3. **Layer normalization:** Stabilizes training dynamics\n",
    "4. **Massive scale:** Billions of parameters trained on trillions of tokens\n",
    "\n",
    "### Modern Improvements Philosophy:\n",
    "\n",
    "1. **Efficiency:** RMSNorm reduces computation without hurting performance\n",
    "2. **Expressiveness:** SwiGLU provides richer feature transformations\n",
    "3. **Generalization:** RoPE improves extrapolation to longer sequences\n",
    "4. **Stability:** Learning rate schedules prevent training collapse\n",
    "\n",
    "### The Engineering Reality:\n",
    "\n",
    "- Many architectural choices are **empirically validated**, not theoretically proven\n",
    "- Small changes can have significant impacts (e.g., pre-norm vs post-norm)\n",
    "- Scaling laws emerge: bigger models + more data = better performance\n",
    "- Implementation details matter enormously for training stability\n",
    "\n",
    "---\n",
    "\n",
    "## Final Architecture Comparison\n",
    "\n",
    "### Baseline GPT-2:\n",
    "- Learned positional embeddings\n",
    "- LayerNorm\n",
    "- Standard MLP with GELU\n",
    "- Constant learning rate (or simple decay)\n",
    "- **124M parameters**\n",
    "\n",
    "### Modern Improved Version:\n",
    "- Rotary positional embeddings (RoPE)\n",
    "- RMSNorm\n",
    "- SwiGLU gated feed-forward\n",
    "- Warmup + cosine decay schedule\n",
    "- **152M parameters**\n",
    "- ~7% faster per step (RMSNorm savings)\n",
    "- Better performance on downstream tasks\n",
    "- Better extrapolation to longer sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "653fcea0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "DEMONSTRATION 1: Attention Score Computation\n",
      "======================================================================\n",
      "\n",
      "Attention scores (before masking):\n",
      "tensor([[0.5000, 0.5000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.5000],\n",
      "        [0.5000, 0.5000, 0.5000]])\n",
      "\n",
      "Interpretation:\n",
      "  Row i, Col j = similarity between token i's query and token j's key\n",
      "\n",
      "After causal masking:\n",
      "tensor([[0.5000,   -inf,   -inf],\n",
      "        [0.0000, 0.0000,   -inf],\n",
      "        [0.5000, 0.5000, 0.5000]])\n",
      "\n",
      "Attention weights (after softmax):\n",
      "tensor([[1.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333]])\n",
      "\n",
      "Note: Each row sums to 1.0, representing probability distribution\n",
      "\n",
      "======================================================================\n",
      "DEMONSTRATION 2: Multi-Head Attention Reshaping\n",
      "======================================================================\n",
      "\n",
      "Original shape: torch.Size([2, 4, 8])\n",
      "After view: torch.Size([2, 4, 2, 4]) [batch, seq, heads, head_dim]\n",
      "After transpose: torch.Size([2, 2, 4, 4]) [batch, heads, seq, head_dim]\n",
      "\n",
      "Now we can process each head in parallel via batched operations!\n",
      "\n",
      "======================================================================\n",
      "DEMONSTRATION 3: RMSNorm vs LayerNorm\n",
      "======================================================================\n",
      "\n",
      "Input: tensor([[1., 2., 3., 4.]])\n",
      "LayerNorm output: tensor([[-1.3416, -0.4472,  0.4472,  1.3416]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "RMSNorm output: tensor([[0.3651, 0.7303, 1.0954, 1.4606]], grad_fn=<MulBackward0>)\n",
      "\n",
      "LayerNorm operations: subtract mean, divide by std\n",
      "RMSNorm operations: divide by RMS (skips mean subtraction)\n",
      "\n",
      "======================================================================\n",
      "DEMONSTRATION 4: SwiGLU Gating Mechanism\n",
      "======================================================================\n",
      "\n",
      "Input: tensor([[1., 2., 3., 4.]])\n",
      "Path 1 (will be gated): tensor([[2., 4., 6., 8.]], grad_fn=<MmBackward0>)\n",
      "Path 2 (signal): tensor([[1., 2., 3., 4.]], grad_fn=<MmBackward0>)\n",
      "\n",
      "Gate (after SiLU): tensor([[1.7616, 3.9281, 5.9852, 7.9973]], grad_fn=<MulBackward0>)\n",
      "Final output (gate × signal): tensor([[ 1.7616,  7.8561, 17.9555, 31.9893]], grad_fn=<MulBackward0>)\n",
      "\n",
      "Notice: Gate values control how much signal passes through\n",
      "\n",
      "======================================================================\n",
      "DEMONSTRATION 5: 2D Rotation for RoPE\n",
      "======================================================================\n",
      "\n",
      "Original point: (1.0, 0.0)\n",
      "After 45° rotation: (0.707, 0.707)\n",
      "  Magnitude: 1.000 → 1.000 (preserved!)\n",
      "After 90° rotation: (0.000, 1.000)\n",
      "  Magnitude: 1.000 → 1.000 (preserved!)\n",
      "After 180° rotation: (-1.000, 0.000)\n",
      "  Magnitude: 1.000 → 1.000 (preserved!)\n",
      "\n",
      "Key insight: Rotation changes direction but preserves magnitude\n",
      "\n",
      "======================================================================\n",
      "DEMONSTRATION 6: RoPE Relative Position Encoding\n",
      "======================================================================\n",
      "\n",
      "Inverse frequency: tensor([1.])\n",
      "\n",
      "Rotation angles (position × frequency):\n",
      "tensor([[0.],\n",
      "        [1.],\n",
      "        [2.],\n",
      "        [3.],\n",
      "        [4.]])\n",
      "\n",
      "Token at position 2: rotation angle = 2.0000\n",
      "Token at position 4: rotation angle = 4.0000\n",
      "Relative angle (encodes distance 2): 2.0000\n",
      "\n",
      "The dot product between rotated vectors naturally depends on\n",
      "the relative position difference!\n",
      "\n",
      "======================================================================\n",
      "DEMONSTRATION 7: Learning Rate Schedules\n",
      "======================================================================\n",
      "\n",
      "Learning rate at different steps:\n",
      "Step    0: warmup=0.000010, cosine=0.000010\n",
      "Step   50: warmup=0.000510, cosine=0.000510\n",
      "Step  100: warmup=0.001000, cosine=0.001000\n",
      "Step  500: warmup=0.001000, cosine=0.000628\n",
      "Step  900: warmup=0.001000, cosine=0.000127\n",
      "Step  999: warmup=0.001000, cosine=0.000100\n",
      "\n",
      "Pattern:\n",
      "  Steps 0-100: LR increases (warmup)\n",
      "  Steps 100+: LR decreases smoothly (cosine decay)\n",
      "\n",
      "======================================================================\n",
      "DEMONSTRATION 8: Complete Mini Forward Pass\n",
      "======================================================================\n",
      "\n",
      "Input shape: [batch=1, seq_len=3, dim=8]\n",
      "After embeddings: torch.Size([1, 3, 8])\n",
      "After LayerNorm: torch.Size([1, 3, 8])\n",
      "Q, K, V (multi-head): torch.Size([1, 2, 3, 4])\n",
      "Attention scores: torch.Size([1, 2, 3, 3])\n",
      "Attention output: torch.Size([1, 2, 3, 4])\n",
      "After concatenating heads: torch.Size([1, 3, 8])\n",
      "After residual: torch.Size([1, 3, 8])\n",
      "After MLP: torch.Size([1, 3, 8])\n",
      "Block output: torch.Size([1, 3, 8])\n",
      "\n",
      "======================================================================\n",
      "All demonstrations complete!\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Code Demonstrations: Understanding Transformer Components\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"DEMONSTRATION 1: Attention Score Computation\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Simple 3-token example\n",
    "B, T, C = 1, 3, 4  # 1 batch, 3 tokens, 4 dimensions\n",
    "\n",
    "# Create simple Q and K matrices\n",
    "q = torch.tensor([\n",
    "    [[1.0, 0.0, 0.0, 0.0],   # Token 0 query\n",
    "     [0.0, 1.0, 0.0, 0.0],   # Token 1 query\n",
    "     [1.0, 1.0, 0.0, 0.0]]   # Token 2 query\n",
    "])\n",
    "\n",
    "k = torch.tensor([\n",
    "    [[1.0, 0.0, 0.0, 0.0],   # Token 0 key\n",
    "     [1.0, 0.0, 0.0, 0.0],   # Token 1 key\n",
    "     [0.0, 1.0, 0.0, 0.0]]   # Token 2 key\n",
    "])\n",
    "\n",
    "# Compute attention scores\n",
    "att = q @ k.transpose(-2, -1) / math.sqrt(C)\n",
    "print(\"\\nAttention scores (before masking):\")\n",
    "print(att[0])\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"  Row i, Col j = similarity between token i's query and token j's key\")\n",
    "\n",
    "# Apply causal mask\n",
    "mask = torch.tril(torch.ones(T, T))\n",
    "att_masked = att.masked_fill(mask == 0, float('-inf'))\n",
    "print(\"\\nAfter causal masking:\")\n",
    "print(att_masked[0])\n",
    "\n",
    "# Apply softmax\n",
    "att_weights = F.softmax(att_masked, dim=-1)\n",
    "print(\"\\nAttention weights (after softmax):\")\n",
    "print(att_weights[0])\n",
    "print(\"\\nNote: Each row sums to 1.0, representing probability distribution\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"DEMONSTRATION 2: Multi-Head Attention Reshaping\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Starting tensor\n",
    "B, T, C = 2, 4, 8  # 2 batch, 4 tokens, 8 dimensions\n",
    "n_head = 2\n",
    "\n",
    "x = torch.randn(B, T, C)\n",
    "print(f\"\\nOriginal shape: {x.shape}\")\n",
    "\n",
    "# Reshape for multi-head\n",
    "x_multihead = x.view(B, T, n_head, C // n_head)\n",
    "print(f\"After view: {x_multihead.shape} [batch, seq, heads, head_dim]\")\n",
    "\n",
    "# Transpose to put heads second\n",
    "x_transposed = x_multihead.transpose(1, 2)\n",
    "print(f\"After transpose: {x_transposed.shape} [batch, heads, seq, head_dim]\")\n",
    "\n",
    "print(\"\\nNow we can process each head in parallel via batched operations!\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"DEMONSTRATION 3: RMSNorm vs LayerNorm\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, dim, eps=1e-8):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(dim))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Calculate RMS along last dimension\n",
    "        rms = torch.sqrt(torch.mean(x ** 2, dim=-1, keepdim=True) + self.eps)\n",
    "        # Normalize\n",
    "        x_norm = x / rms\n",
    "        # Apply learned scaling\n",
    "        return x_norm * self.weight\n",
    "\n",
    "# Test both normalizations\n",
    "x = torch.tensor([[1.0, 2.0, 3.0, 4.0]])\n",
    "\n",
    "layernorm = nn.LayerNorm(4)\n",
    "rmsnorm = RMSNorm(4)\n",
    "\n",
    "ln_out = layernorm(x)\n",
    "rms_out = rmsnorm(x)\n",
    "\n",
    "print(\"\\nInput:\", x)\n",
    "print(\"LayerNorm output:\", ln_out)\n",
    "print(\"RMSNorm output:\", rms_out)\n",
    "\n",
    "print(\"\\nLayerNorm operations: subtract mean, divide by std\")\n",
    "print(\"RMSNorm operations: divide by RMS (skips mean subtraction)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"DEMONSTRATION 4: SwiGLU Gating Mechanism\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Simple gating example\n",
    "x = torch.tensor([[1.0, 2.0, 3.0, 4.0]])\n",
    "\n",
    "# Two parallel paths\n",
    "w1 = nn.Linear(4, 4, bias=False)\n",
    "w2 = nn.Linear(4, 4, bias=False)\n",
    "\n",
    "# Initialize with specific weights for demonstration\n",
    "with torch.no_grad():\n",
    "    w1.weight.copy_(torch.eye(4) * 2)  # Doubles input\n",
    "    w2.weight.copy_(torch.eye(4))       # Identity\n",
    "\n",
    "hidden1 = w1(x)\n",
    "hidden2 = w2(x)\n",
    "\n",
    "print(\"\\nInput:\", x)\n",
    "print(\"Path 1 (will be gated):\", hidden1)\n",
    "print(\"Path 2 (signal):\", hidden2)\n",
    "\n",
    "# Apply SiLU to path 1 (gate)\n",
    "gate = hidden1 * torch.sigmoid(hidden1)\n",
    "print(\"\\nGate (after SiLU):\", gate)\n",
    "\n",
    "# Element-wise multiplication\n",
    "output = gate * hidden2\n",
    "print(\"Final output (gate × signal):\", output)\n",
    "\n",
    "print(\"\\nNotice: Gate values control how much signal passes through\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"DEMONSTRATION 5: 2D Rotation for RoPE\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "def rotate_2d(x, y, theta):\n",
    "    \"\"\"Rotate point (x,y) by angle theta\"\"\"\n",
    "    cos_theta = math.cos(theta)\n",
    "    sin_theta = math.sin(theta)\n",
    "    \n",
    "    x_new = x * cos_theta - y * sin_theta\n",
    "    y_new = x * sin_theta + y * cos_theta\n",
    "    \n",
    "    return x_new, y_new\n",
    "\n",
    "# Start with point on x-axis\n",
    "x, y = 1.0, 0.0\n",
    "print(f\"\\nOriginal point: ({x}, {y})\")\n",
    "\n",
    "# Rotate by different angles\n",
    "for angle_deg in [45, 90, 180]:\n",
    "    angle_rad = math.radians(angle_deg)\n",
    "    x_rot, y_rot = rotate_2d(x, y, angle_rad)\n",
    "    print(f\"After {angle_deg}° rotation: ({x_rot:.3f}, {y_rot:.3f})\")\n",
    "    \n",
    "    # Check magnitude preserved\n",
    "    original_mag = math.sqrt(x**2 + y**2)\n",
    "    rotated_mag = math.sqrt(x_rot**2 + y_rot**2)\n",
    "    print(f\"  Magnitude: {original_mag:.3f} → {rotated_mag:.3f} (preserved!)\")\n",
    "\n",
    "print(\"\\nKey insight: Rotation changes direction but preserves magnitude\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"DEMONSTRATION 6: RoPE Relative Position Encoding\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Simulate RoPE for 2 dimensions\n",
    "dim = 2\n",
    "base_freq = 10000\n",
    "\n",
    "# Compute frequency\n",
    "inv_freq = 1.0 / (base_freq ** (torch.arange(0, dim, 2).float() / dim))\n",
    "print(f\"\\nInverse frequency: {inv_freq}\")\n",
    "\n",
    "# Create rotation angles for different positions\n",
    "positions = torch.arange(5)  # Positions 0-4\n",
    "freqs = torch.outer(positions, inv_freq)\n",
    "\n",
    "print(\"\\nRotation angles (position × frequency):\")\n",
    "print(freqs)\n",
    "\n",
    "# Simulate dot product between rotated q and k at different positions\n",
    "pos_i, pos_j = 2, 4  # Token at position 2 attends to position 4\n",
    "\n",
    "angle_i = freqs[pos_i, 0]\n",
    "angle_j = freqs[pos_j, 0]\n",
    "relative_angle = angle_j - angle_i\n",
    "\n",
    "print(f\"\\nToken at position {pos_i}: rotation angle = {angle_i:.4f}\")\n",
    "print(f\"Token at position {pos_j}: rotation angle = {angle_j:.4f}\")\n",
    "print(f\"Relative angle (encodes distance {pos_j - pos_i}): {relative_angle:.4f}\")\n",
    "\n",
    "print(\"\\nThe dot product between rotated vectors naturally depends on\")\n",
    "print(\"the relative position difference!\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"DEMONSTRATION 7: Learning Rate Schedules\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "def linear_warmup(step, warmup_steps, base_lr):\n",
    "    if step < warmup_steps:\n",
    "        return base_lr * (step + 1) / warmup_steps\n",
    "    return base_lr\n",
    "\n",
    "def cosine_decay(step, warmup_steps, total_steps, max_lr, min_lr):\n",
    "    if step < warmup_steps:\n",
    "        return max_lr * (step + 1) / warmup_steps\n",
    "    progress = (step - warmup_steps) / (total_steps - warmup_steps)\n",
    "    return min_lr + 0.5 * (max_lr - min_lr) * (1 + math.cos(math.pi * progress))\n",
    "\n",
    "# Simulate training schedule\n",
    "warmup_steps = 100\n",
    "total_steps = 1000\n",
    "base_lr = 0.001\n",
    "\n",
    "print(\"\\nLearning rate at different steps:\")\n",
    "for step in [0, 50, 100, 500, 900, 999]:\n",
    "    lr_warmup = linear_warmup(step, warmup_steps, base_lr)\n",
    "    lr_cosine = cosine_decay(step, warmup_steps, total_steps, base_lr, base_lr * 0.1)\n",
    "    print(f\"Step {step:4d}: warmup={lr_warmup:.6f}, cosine={lr_cosine:.6f}\")\n",
    "\n",
    "print(\"\\nPattern:\")\n",
    "print(\"  Steps 0-100: LR increases (warmup)\")\n",
    "print(\"  Steps 100+: LR decreases smoothly (cosine decay)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"DEMONSTRATION 8: Complete Mini Forward Pass\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Tiny transformer block simulation\n",
    "B, T, C = 1, 3, 8\n",
    "n_head = 2\n",
    "head_dim = C // n_head\n",
    "\n",
    "print(f\"\\nInput shape: [batch={B}, seq_len={T}, dim={C}]\")\n",
    "\n",
    "# 1. Input\n",
    "x = torch.randn(B, T, C)\n",
    "print(f\"After embeddings: {x.shape}\")\n",
    "\n",
    "# 2. Layer norm\n",
    "ln = nn.LayerNorm(C)\n",
    "x_norm = ln(x)\n",
    "print(f\"After LayerNorm: {x_norm.shape}\")\n",
    "\n",
    "# 3. Attention (simplified)\n",
    "qkv_proj = nn.Linear(C, 3 * C)\n",
    "qkv = qkv_proj(x_norm)\n",
    "q, k, v = qkv.chunk(3, dim=-1)\n",
    "\n",
    "# Reshape for multi-head\n",
    "q = q.view(B, T, n_head, head_dim).transpose(1, 2)\n",
    "k = k.view(B, T, n_head, head_dim).transpose(1, 2)\n",
    "v = v.view(B, T, n_head, head_dim).transpose(1, 2)\n",
    "print(f\"Q, K, V (multi-head): {q.shape}\")\n",
    "\n",
    "# Attention scores\n",
    "att = (q @ k.transpose(-2, -1)) / math.sqrt(head_dim)\n",
    "print(f\"Attention scores: {att.shape}\")\n",
    "\n",
    "# Causal mask\n",
    "mask = torch.tril(torch.ones(T, T))\n",
    "att = att.masked_fill(mask == 0, float('-inf'))\n",
    "att = F.softmax(att, dim=-1)\n",
    "\n",
    "# Aggregate values\n",
    "out = att @ v\n",
    "print(f\"Attention output: {out.shape}\")\n",
    "\n",
    "# Reshape back\n",
    "out = out.transpose(1, 2).contiguous().view(B, T, C)\n",
    "print(f\"After concatenating heads: {out.shape}\")\n",
    "\n",
    "# 4. Residual connection\n",
    "x = x + out\n",
    "print(f\"After residual: {x.shape}\")\n",
    "\n",
    "# 5. MLP\n",
    "x_norm2 = ln(x)\n",
    "mlp = nn.Sequential(\n",
    "    nn.Linear(C, 4 * C),\n",
    "    nn.GELU(),\n",
    "    nn.Linear(4 * C, C)\n",
    ")\n",
    "mlp_out = mlp(x_norm2)\n",
    "print(f\"After MLP: {mlp_out.shape}\")\n",
    "\n",
    "# 6. Final residual\n",
    "x = x + mlp_out\n",
    "print(f\"Block output: {x.shape}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"All demonstrations complete!\")\n",
    "print(\"=\" * 70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
